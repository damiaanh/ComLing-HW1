import string
import nltk
import re

class BigramModel:
    """
    BigramModel takes a tokenized corpus and greates bigramcount and wordcounts. It also crreates a dicionary woth all unique 
    bigrams and words, with their frequency.
    Authors: Binti Dekker, Damiaan Houtschild
    """

    def __init__(self, tokens):
        """
        Creates wordlists and frequency tables for unique words and bigrams.
        """
        self.tokens = tokens
        self.wordcount = {}
        self.tokenlist = self.clean_tokens(self.tokens)

        self.words = self.count_word_freqs(self.tokenlist)
        self.totalwords = self.count_total(self.words)

        self.bigrams = self.count_bigram_freqs(self.tokenlist)
        self.totalbigrams = self.count_total(self.bigrams)

    def clean_tokens(self, tokens):
        """
        Prepares the tokens generated by CorpusReader by removing punctuation,
        lower case all and add start and end markings.
        Returns a list of tokenized lists (sentences).
        """
        STARTMARK = "<s>"
        ENDMARK = "</s>"
        cleanlist = []
        for sentence in tokens:
            cleansent = []
            cleansent.append(STARTMARK)
            for token in sentence:
                new = token.strip(string.punctuation)
                new = new.lower()
                if new != "":     # Only continue for tokens that are not empty (due to removing punctuation)
                    cleansent.append(new)
            cleansent.append(ENDMARK)
            cleanlist.append(cleansent)
        return cleanlist


    def count_word_freqs(self, sentences):
        """
        Calculates frequencies for all words in all sentences.
        Returns a dictionary with all (unique) words and their frequency.
        """
        wordcount = {}
        for sentence in sentences:
            for word in sentence:
                if word in wordcount:
                    wordcount[word] += 1
                else:
                    wordcount[word] = 1
        return wordcount


    def count_total(self, freqdict):
        """
        Counts the total amount of items in a dataset by adding all frequencies.
        Returns the total as integer.
        """
        total = 0
        for item in freqdict:
            total += freqdict[item]
        return total

    
    def count_bigram_freqs(self, sentences):
        """
        Calculates frequencies for all bigrams in all sentences.
        Returns a dictionary with all (unique) bigrams and their frequency.
        """
        bigramcount = {}
        for sentence in sentences:
            for word in range(len(sentence[:-1])): # Not looping over the last word ("</s>") since there is no second word
                bigram = f"{sentence[word]} {sentence[word+1]}"
                if bigram in bigramcount:
                    bigramcount[bigram] += 1
                else:
                    bigramcount[bigram] = 1
        return bigramcount


    def p_raw(self, w, w_n):
        """
        Takes a word and a successor word and looks if this specific bigram exists.
        If so, it returns the probability for this bigram. Otherwise it returns 0. 
        """
        bigram = f"{w} {w_n}"
        if w in self.words:
            total = self.words[w]
        else:
            return 0
        if bigram in self.bigrams:
            p = self.bigrams[bigram] / total
        else:
            p = 0
        return p
    
    def p_smooth(self,w, w_n):
        """
        Return P(w_n | w) , the Laplace-smoothed probability of
        seeing token w_n if we have just seen token w.
        """      
        bigram = f"{w} {w_n}"
        if w in self.tokens and w_n in self.tokens: 
            p = (self.bigrams[bigram] + 1) / self.totalbigrams_smooth()
        else: 
            p = 0 #als de woorden niet bestaan is er ook geen smooth functie
        return p

    def successors(self, w):
        """
        Returns a list of pairs (w_i, c_i) , where w_i is a token that
        might follow w , and c_i is its raw (unsmoothed) bigram
        probability, P_r(w_i | w).
        """
        wi_ci_list = []
        for successor in self.words:
            p = self.p_raw(w, successor)
            if p != 0:
                wi_ci_list.append([successor, p])
        return wi_ci_list
            
    def perplexity(self, sent):
        sent = [sent.split()]
        print(sent)
        tokenlist = self.clean_tokens(sent)
        calc = 0
        for sentence in tokenlist:
            for token in range(len(sentence[:-1])):
                p = self.p_smooth(sentence[token], sentence[token + 1])
                if p != 0:
                    calc *= 1/p
        perplxity = pow(calc, (1/self.count_bigram_freqs(z)))
        return perplxity

